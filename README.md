# Talks
Slides, Code, Data, and Instructions

# Purpose


# PoC Steps
1. Follow these instructions http://goo.gl/cAEQtD to create a Hadoop cluster with Spark, Parquet, and Python
2. Run the code in TalkDemo_HadoopHivePySpark.txt to load in the test dataset and create your base tables
3. Fire up PySpark in Jupyter and step through each line of Spark code with your test dataset
4. Peruse the slides OpenDataSci - Using Spark Python Parquet - Talk - 20150530.pdf